{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nutshell import ModelData, Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fathom concepts\n",
    "\n",
    "# FactSet is a dataset with three columns: subject, fact, value\n",
    "# You can generate a FactSet from a data with rows and columns using \n",
    "#  source csv has one row per subject - with columns containing fact values\n",
    "#  destination dataset has one row per subject/fact\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactSet:\n",
    "    \n",
    "    def __init__(self):\n",
    "        fact_data = pd.DataFrame()   # internal dataset with one row per subject/fact\n",
    "        label_data = pd.DataFrame()  # internal dataset with one row per subject\n",
    "        model_name = 'factset'       # override this value to the name of the model file that will be built\n",
    "        model = None\n",
    "        subject_column = ''          # name of subject id column\n",
    "        fact_name_column = ''\n",
    "        fact_value_column = ''\n",
    "        label_column = ''            # name label column - this will be not be used in training\n",
    "        fact_colnames = []           # optional: list of fact columns; default is all non subject/label columns\n",
    "               \n",
    "    def load_subject_rows(self, df):\n",
    "        \n",
    "        # dataset is in the format one row per subject, one fact per column\n",
    "        print('Transposing data into fact rows...')\n",
    "        \n",
    "        #to do\n",
    "        \n",
    "    \n",
    "    def load_fact_rows(self, df):\n",
    "               \n",
    "        # dataset is in the format one row per subject/fact, each row has a fact name and value column\n",
    "        self.fact_data = pd.DataFrame()\n",
    "        self.fact_data['subject'] = df[self.subject_column]\n",
    "        self.fact_data['fact_name'] = df[self.fact_name_column]\n",
    "        self.fact_data['fact_value'] = df[self.fact_value_column]\n",
    "        \n",
    "        print(len(df), 'fact rows loaded')\n",
    "    \n",
    "    def learn(self, model_name=''):\n",
    "     \n",
    "        # training a neural net to tell which facts are true and which are false about each subject\n",
    "        #  so for every true fact about a subject, there should be an equal number of false facts\n",
    "        #  false facts are taken from other subjects, so they are plausible facts\n",
    "    \n",
    "        if model_name == '':\n",
    "            model_name = self.model_name\n",
    "    \n",
    "        \n",
    "        print('Preparing data for model training...')\n",
    "\n",
    "        dfLearn = pd.DataFrame()\n",
    "        dfLearn['subject'] = self.fact_data['subject']\n",
    "        dfLearn['fact'] = self.fact_data['fact_name'].astype('str') + '/' + self.fact_data['fact_value'].astype('str')\n",
    "        dfLearn['is_true'] = 1\n",
    "        \n",
    "        data = ModelData(dfLearn)\n",
    "        data.category_columns = ['subject','fact']\n",
    "        data.label_column = 'is_true'\n",
    "        data.prepare_data()\n",
    "        data.add_false_rows(['subject'])\n",
    "                \n",
    "        print('Building neural network...')\n",
    "        \n",
    "        model = Learner(data)\n",
    "        model.dropout = 0 # overfitting is the point\n",
    "        model.batch_size = 1024\n",
    "        model.build_model()\n",
    "        \n",
    "        print('Training neural network...')\n",
    "\n",
    "        model.train_model(model_name, epochs=3, super_epochs=3)\n",
    "\n",
    "        print ('Stored model to: ', model_name)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as a sample, use instacart purchases\n",
    "# we will use users as subjects and products purchased as facts\n",
    "\n",
    "dataPath = '/Users/Miles/Documents/Datasets/Instacart/'\n",
    "dfProducts = pd.read_csv(dataPath + 'products.csv')\n",
    "dfOrders = pd.read_csv(dataPath + 'orders.csv')\n",
    "dfOrderProducts = pd.read_csv(dataPath + 'order_products__prior.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full set has 206K unique users\n",
    "dfOrders['user_id'].nunique()\n",
    "\n",
    "# lets start with a sample set of 2000 users\n",
    "dfSample = pd.DataFrame()\n",
    "dfSample['user_id'] = dfOrders['user_id'].unique()\n",
    "dfSample = dfSample.sample(n=2000)\n",
    "\n",
    "dfOrders = pd.merge(dfSample, dfOrders, on='user_id').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of user ids and products they have ordered\n",
    "\n",
    "dfUserProducts = pd.merge(dfOrders, dfOrderProducts, on='order_id')[['user_id', 'product_id']].\\\n",
    "    sort_values(['user_id', 'product_id']).groupby(['user_id', 'product_id']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUserProducts['fact_name'] = 'product'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126878 fact rows loaded\n"
     ]
    }
   ],
   "source": [
    "facts = FactSet()\n",
    "facts.subject_column = 'user_id'\n",
    "facts.fact_name_column = 'fact_name'\n",
    "facts.fact_value_column = 'product_id'\n",
    "facts.model_name = 'grocery'\n",
    "facts.load_fact_rows(dfUserProducts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "elf = facts\n",
    "\n",
    "dfLearn = pd.DataFrame()\n",
    "dfLearn['subject'] = elf.fact_data['subject']\n",
    "dfLearn['fact'] = elf.fact_data['fact_name'].astype('str') + '/' + elf.fact_data['fact_value'].astype('str')\n",
    "dfLearn['is_true'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>fact</th>\n",
       "      <th>is_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>product/2138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>product/3108</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>product/3243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>product/3518</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>product/3873</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject          fact  is_true\n",
       "0       23  product/2138        1\n",
       "1       23  product/3108        1\n",
       "2       23  product/3243        1\n",
       "3       23  product/3518        1\n",
       "4       23  product/3873        1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfLearn[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing category columns...\n",
      "subject 2000 unique values\n",
      "fact 20547 unique values\n",
      "Done preparing data\n",
      "Adding false rows\n",
      "Added 126878 rows\n"
     ]
    }
   ],
   "source": [
    "data = ModelData(dfLearn)\n",
    "data.category_columns = ['subject','fact']\n",
    "data.label_column = 'is_true'\n",
    "data.prepare_data()\n",
    "data.add_false_rows(['subject'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing category columns...\n",
      "** Using pre-defined token map **\n",
      "subject 2002 unique values\n",
      "fact 20549 unique values\n",
      "Done preparing data\n",
      "Training examples: 114191\n",
      "Validation examples: 12687\n"
     ]
    }
   ],
   "source": [
    "data.prepare_data()\n",
    "data.split_data(shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Sequential Merge Layer Shape: (?, 100)\n",
      "Loss Function: mse\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_subject (InputLayer)       (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_fact (InputLayer)          (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embed_subject (Embedding)        (None, 1, 50)         100100      input_subject[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "embed_fact (Embedding)           (None, 1, 50)         1027450     input_fact[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "reshape_subject (Reshape)        (None, 50)            0           embed_subject[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "reshape_fact (Reshape)           (None, 50)            0           embed_fact[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "merge_non_sequential (Concatenat (None, 100)           0           reshape_subject[0][0]            \n",
      "                                                                   reshape_fact[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_0 (Dense)                  (None, 50)            5050        merge_non_sequential[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_dropout_0 (Dropout)        (None, 50)            0           dense_0[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 50)            2550        dense_dropout_0[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_dropout_1 (Dropout)        (None, 50)            0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_reshape (Reshape)          (None, 50)            0           dense_dropout_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_representation (Dense)     (None, 50)            2550        dense_reshape[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_output (Dense)             (None, 1)             51          dense_representation[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 1,137,751\n",
      "Trainable params: 1,137,751\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Super Epoch: 1\n",
      "Learning Rate: 0.001\n",
      "Train on 114191 samples, validate on 12687 samples\n",
      "Epoch 1/3\n",
      "114191/114191 [==============================] - 4s - loss: 0.1253 - acc: 0.8408 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 2/3\n",
      "114191/114191 [==============================] - 3s - loss: 0.0078 - acc: 1.0000 - val_loss: 2.9089e-04 - val_acc: 1.0000\n",
      "Epoch 3/3\n",
      "114191/114191 [==============================] - 3s - loss: 0.0048 - acc: 1.0000 - val_loss: 1.2950e-04 - val_acc: 1.0000\n",
      "Super Epoch: 2\n",
      "Learning Rate: 0.0001\n",
      "Train on 114191 samples, validate on 12687 samples\n",
      "Epoch 1/3\n",
      "114191/114191 [==============================] - 4s - loss: 0.0036 - acc: 1.0000 - val_loss: 1.0244e-04 - val_acc: 1.0000\n",
      "Epoch 2/3\n",
      "114191/114191 [==============================] - 3s - loss: 0.0029 - acc: 1.0000 - val_loss: 5.9052e-05 - val_acc: 1.0000\n",
      "Epoch 3/3\n",
      "114191/114191 [==============================] - 3s - loss: 0.0022 - acc: 1.0000 - val_loss: 3.6403e-05 - val_acc: 1.0000\n",
      "Super Epoch: 3\n",
      "Learning Rate: 1e-05\n",
      "Train on 114191 samples, validate on 12687 samples\n",
      "Epoch 1/3\n",
      "114191/114191 [==============================] - 4s - loss: 0.0016 - acc: 1.0000 - val_loss: 1.4696e-05 - val_acc: 1.0000\n",
      "Epoch 2/3\n",
      "114191/114191 [==============================] - 3s - loss: 0.0011 - acc: 1.0000 - val_loss: 2.0774e-05 - val_acc: 1.0000\n",
      "Epoch 3/3\n",
      "114191/114191 [==============================] - 3s - loss: 7.3081e-04 - acc: 1.0000 - val_loss: 1.3938e-05 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model_name = 'grocery'\n",
    "model = Learner(data)\n",
    "model.dropout = 0 # overfitting is the point\n",
    "model.batch_size = 1024\n",
    "model.build_model()\n",
    "model.train_model(model_name, epochs=3, super_epochs=3)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
